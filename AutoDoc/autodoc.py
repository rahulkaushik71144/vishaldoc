import os
import shutil
import stat
from git import Repo
from datetime import datetime
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch
from pylatex import Document, Section, Subsection, Subsubsection, Command, Package
from pylatex.utils import NoEscape, bold

class RepoDocBuilder:
    def __init__(self):
        self.summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
        self.ner = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english")
        self.stop_words = set(stopwords.words('english'))
        
        # Load a smaller open-source LLM for text generation
        model_name = "gpt2"  # You can replace this with a more suitable open-source model
        self.text_generator = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # Add a padding token if it's missing
        if self.tokenizer.pad_token is None:
            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            self.text_generator.resize_token_embeddings(len(self.tokenizer))

    def generate_documentation(self, input_data, mode='active'):
        try:
            if mode == 'active':
                return self.process_github_repo(input_data)
            elif mode == 'passive':
                return self.process_project_description(input_data)
            else:
                raise ValueError("Invalid mode. Choose 'active' or 'passive'.")
        except Exception as e:
            print(f"An error occurred: {str(e)}")
            return None

    def process_github_repo(self, repo_url):
        repo_name = repo_url.split("/")[-1]
        temp_path = f"temp/{repo_name}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        repo = Repo.clone_from(repo_url, temp_path)

        try:
            readme_content = self.get_readme_content(temp_path)
            code_files = self.get_code_files(temp_path)
            directory_structure = self.get_directory_structure(temp_path)

            doc = Document()
            doc.packages.append(Package('hyperref'))
            doc.preamble.append(Command('title', f'Documentation for {repo_name}'))
            doc.preamble.append(Command('author', 'Generated by RepoDoc Builder'))
            doc.append(NoEscape(r'\maketitle'))
            doc.append(NoEscape(r'\tableofcontents'))

            with doc.create(Section('Project Overview')):
                if readme_content:
                    doc.append(self.summarizer(readme_content, max_length=300, min_length=100)[0]['summary_text'])
                else:
                    doc.append("No README file found in the repository.")

            with doc.create(Section('Directory Structure')):
                doc.append(NoEscape(r'\begin{verbatim}'))
                doc.append(directory_structure)
                doc.append(NoEscape(r'\end{verbatim}'))

            with doc.create(Section('Code Analysis')):
                for file in code_files:
                    with doc.create(Subsection(file['name'])):
                        doc.append(self.analyze_code_file(file))

            with doc.create(Section('API Documentation')):
                doc.append(self.generate_api_documentation(code_files))

            with doc.create(Section('Future Recommendations')):
                doc.append(self.generate_recommendations(readme_content, code_files))

            return doc
        finally:
            # Ensure we clean up the temporary directory, handling readonly files
            shutil.rmtree(temp_path, ignore_errors=False, onerror=self.remove_readonly)

    @staticmethod
    def remove_readonly(func, path, excinfo):
        "Clear the readonly bit and reattempt the removal"
        os.chmod(path, stat.S_IWRITE)
        func(path)


    def process_project_description(self, description):
        doc = Document()
        doc.packages.append(Package('hyperref'))
        doc.preamble.append(Command('title', 'Project Documentation'))
        doc.preamble.append(Command('author', 'Generated by RepoDoc Builder'))
        doc.append(NoEscape(r'\maketitle'))
        doc.append(NoEscape(r'\tableofcontents'))

        with doc.create(Section('Project Overview')):
            doc.append(self.summarizer(description, max_length=300, min_length=100)[0]['summary_text'])

        with doc.create(Section('Key Concepts')):
            concepts = self.extract_key_concepts(description)
            for concept in concepts:
                doc.append(f"â€¢ {concept}\n")

        with doc.create(Section('Potential Architecture')):
            doc.append(self.generate_architecture_suggestion(description))

        return doc

    def get_readme_content(self, repo_path):
        try:
            readme_path = os.path.join(repo_path, "README.md")
            with open(readme_path, 'r', encoding='utf-8') as f:
                return f.read()
        except:
            print("No README found.")
            return ""

    def get_code_files(self, repo_path):
        code_files = []
        for root, dirs, files in os.walk(repo_path):
            dirs[:] = [d for d in dirs if d not in ['.git', 'node_modules', 'venv', '__pycache__']]
            for file in files:
                if file.endswith(('.py', '.js', '.java', '.cpp', '.h', '.cs', '.html', '.css')):
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                        code_files.append({
                            'name': os.path.relpath(file_path, repo_path),
                            'content': content
                        })
                    except Exception as e:
                        print(f"Error reading file {file_path}: {str(e)}")
        return code_files

    def get_directory_structure(self, startpath):
        tree = ""
        for root, dirs, files in os.walk(startpath):
            dirs[:] = [d for d in dirs if d not in ['.git', 'node_modules', 'venv', '__pycache__']]
            level = root.replace(startpath, '').count(os.sep)
            indent = ' ' * 4 * (level)
            tree += f"{indent}{os.path.basename(root)}/\n"
            subindent = ' ' * 4 * (level + 1)
            for f in files:
                tree += f"{subindent}{f}\n"
        return tree

    def analyze_code_file(self, file):
        content = file['content']
        summary = self.summarizer(content, max_length=200, min_length=50)[0]['summary_text']
        
        # Extract functions/classes
        functions_classes = self.extract_functions_classes(content)
        
        doc = Subsection(file['name'])
        doc.append(summary)
        
        with doc.create(Subsubsection('Functions and Classes')):
            for item in functions_classes:
                doc.append(NoEscape(r'\textbf{' + item['name'] + '}'))
                doc.append(item['description'])
                doc.append('\n\n')
        
        return doc

    def extract_functions_classes(self, code):
        # This is a simplified version. In a real-world scenario, you'd want to use
        # more sophisticated code analysis techniques, possibly using AST.
        lines = code.split('\n')
        functions_classes = []
        for line in lines:
            if line.strip().startswith(('def ', 'class ')):
                name = line.split('(')[0].split()[-1]
                description = self.generate_text(f"Describe the purpose of this {line.split()[0]} named {name}:")
                functions_classes.append({
                    'name': name,
                    'description': description
                })
        return functions_classes

    def generate_api_documentation(self, code_files):
        api_doc = ""
        for file in code_files:
            api_doc += f"File: {file['name']}\n\n"
            api_doc += self.extract_api_info(file['content'])
            api_doc += "\n\n"
        return self.generate_text(f"Generate API documentation based on this information:\n\n{api_doc}")

    def extract_api_info(self, code):
        # This is a simplified version. In a real-world scenario, you'd want to use
        # more sophisticated code analysis techniques, possibly using AST.
        lines = code.split('\n')
        api_info = ""
        for line in lines:
            if line.strip().startswith(('def ', 'class ')):
                api_info += line + "\n"
        return api_info

    def generate_recommendations(self, readme, code_files):
        code_summary = "\n".join([self.summarizer(file['content'], max_length=100, min_length=30)[0]['summary_text'] for file in code_files])
        prompt = f"Based on this README:\n\n{readme}\n\nAnd this code summary:\n\n{code_summary}\n\nProvide recommendations for future improvements and potential features:"
        return self.generate_text(prompt)

    def generate_architecture_suggestion(self, description):
        prompt = f"Based on this project description:\n\n{description}\n\nSuggest a potential architecture for this project:"
        return self.generate_text(prompt)

    def extract_key_concepts(self, text):
        words = word_tokenize(text)
        words = [word.lower() for word, pos in pos_tag(words) if pos.startswith('NN') and word.isalnum()]
        words = [word for word in words if word not in self.stop_words]
        
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform([' '.join(words)])
        feature_names = vectorizer.get_feature_names()
        
        important_words = sorted(zip(feature_names, tfidf_matrix.toarray()[0]), key=lambda x: x[1], reverse=True)[:15]
        return [word for word, _ in important_words]

    def generate_text(self, prompt, max_length=500):
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, pad_token_id=self.tokenizer.pad_token_id)
        
        with torch.no_grad():
            outputs = self.text_generator.generate(
                inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_length=max_length,
                num_return_sequences=1,
                temperature=0.7,
                top_k=50,
                top_p=0.95,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id  # Ensure the pad token is used
            )
            
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)


def main():
    doc_builder = RepoDocBuilder()

    while True:
        mode = input("Enter mode (active/passive) or 'quit' to exit: ").lower()
        
        if mode == 'quit':
            break
        
        if mode not in ['active', 'passive']:
            print("Invalid mode. Please enter 'active' or 'passive'.")
            continue
        
        input_data = input("Enter GitHub repo URL (for active mode) or project description (for passive mode): ")
        
        doc = doc_builder.generate_documentation(input_data, mode)
        
        if doc:
            output_filename = input("Enter output filename (without extension): ")
            doc.generate_pdf(output_filename, clean_tex=False)
            print(f"Documentation generated and saved as {output_filename}.pdf")
        else:
            print("Failed to generate documentation.")

if __name__ == "__main__":
    main()